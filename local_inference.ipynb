{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh\n",
    "from jax.sharding import NamedSharding\n",
    "\n",
    "import flax\n",
    "\n",
    "from rich import print\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch_to_flax import torch_to_flax\n",
    "from model_flax import get_partition_rules, Qwen2Config, Qwen2ForCausalLM\n",
    "\n",
    "# Set this environment variable before importing JAX.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9375\"\n",
    "\n",
    "\n",
    "SHARD_MODEL = True\n",
    "\n",
    "\n",
    "def create_sharding_specs(params):\n",
    "\n",
    "    # Get available JAX devices and create mesh\n",
    "    devices = jax.devices()\n",
    "    device_mesh = np.array(devices).reshape(-1)  # 1D mesh\n",
    "    mesh = Mesh(device_mesh, (\"mp\",))\n",
    "    print(mesh)\n",
    "\n",
    "    partition_rules = get_partition_rules()\n",
    "\n",
    "    def assign_spec(path, _):\n",
    "        # Create a slash-separated string from the tuple path\n",
    "        path_str = \"/\".join(map(str, path))\n",
    "        # Look for a matching partition rule\n",
    "        for rule_path, spec in partition_rules:\n",
    "            if rule_path in path_str:\n",
    "                return NamedSharding(mesh, spec)\n",
    "        # If no rule matches, return a default sharding spec\n",
    "        return NamedSharding(mesh, None)\n",
    "\n",
    "    \n",
    "    return jax.tree_util.tree_map_with_path(assign_spec, params)\n",
    "\n",
    "\n",
    "def init_model(SHARD_MODEL:bool):\n",
    "    \"\"\"Initialize a model with parameters on CPU then shard them to GPU after checkpoint loading.\"\"\"\n",
    "    # Create config and model\n",
    "    config = Qwen2Config()\n",
    "    model = Qwen2ForCausalLM(config=config)\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    input_shape = (1, 32)\n",
    "\n",
    "    # Force initialization on CPU to avoid duplicate GPU allocations\n",
    "    with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "        try:\n",
    "            params = model.init(rng, jnp.ones(input_shape, dtype=jnp.int4))\n",
    "        except Exception as e:\n",
    "            params = model.init(rng, jnp.ones(input_shape, dtype=jnp.int32))\n",
    "\n",
    "\n",
    "    # Load the parameters from the file\n",
    "    try:\n",
    "        with open(\"flax_params.msgpack\", \"rb\") as f:\n",
    "            params = {\n",
    "                \"params\": flax.serialization.from_bytes(params[\"params\"], f.read())\n",
    "            }\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Running conversion...\")\n",
    "        torch_to_flax()\n",
    "        with open(\"flax_params.msgpack\", \"rb\") as f:\n",
    "            params = {\n",
    "                \"params\": flax.serialization.from_bytes(params[\"params\"], f.read())\n",
    "            }\n",
    "\n",
    "    # Shard the parameters\n",
    "    if SHARD_MODEL:\n",
    "        sharding_specs = create_sharding_specs(params)\n",
    "\n",
    "        print(sharding_specs)\n",
    "\n",
    "        params = jax.tree_util.tree_map(\n",
    "            lambda x, spec: jax.device_put(x, spec), params, sharding_specs\n",
    "        )\n",
    "    else:\n",
    "        params = jax.device_put(params)\n",
    "\n",
    "\n",
    "    return model, params\n",
    "\n",
    "model, params = init_model(SHARD_MODEL)\n",
    "print(\"Sharded model initialized.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
