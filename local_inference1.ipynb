{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Einstein sum subscript 'bs(hd)' does not contain the correct number of indices for operand 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m, in \u001b[0;36minit_model\u001b[0;34m(SHARD_MODEL)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:562\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__call__\u001b[0;34m(self, input_ids, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:426\u001b[0m, in \u001b[0;36mQwen2Model.__call__\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, deterministic, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    425\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 426\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:332\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.__call__\u001b[0;34m(self, hidden_states, attention_mask, position_ids, deterministic, output_attentions, position_embeddings)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Self-attention computation with GQA, RoPE, and QKV bias.\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Residual connection after attention.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:200\u001b[0m, in \u001b[0;36mQwen2Attention.__call__\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, deterministic, output_attentions)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Use einsum to reshape and transpose.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# For query_states: from [b, s, (nh * hd)] to [b, nh, s, hd]\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb s (h d) -> b h s d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# For key_states and value_states: from [b, s, (nk * cd)] to [b, nk, s, cd]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mechinterp/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:9763\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(subscripts, out, optimize, precision, preferred_element_type, _dot_general, out_sharding, *operands)\u001b[0m\n\u001b[1;32m   9762\u001b[0m \u001b[38;5;66;03m# using einsum_call=True here is an internal api for opt_einsum... sorry\u001b[39;00m\n\u001b[0;32m-> 9763\u001b[0m operands, contractions \u001b[38;5;241m=\u001b[39m \u001b[43mcontract_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9764\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_blas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9766\u001b[0m contractions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((a, \u001b[38;5;28mfrozenset\u001b[39m(b), c) \u001b[38;5;28;01mfor\u001b[39;00m a, b, c, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;129;01min\u001b[39;00m contractions)\n",
      "File \u001b[0;32m~/anaconda3/envs/mechinterp/lib/python3.11/site-packages/opt_einsum/contract.py:312\u001b[0m, in \u001b[0;36mcontract_path\u001b[0;34m(subscripts, use_blas, optimize, memory_limit, shapes, *operands, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sh) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(term):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEinstein sum subscript \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_list[tnum]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not contain the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect number of indices for operand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m     )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cnum, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(term):\n",
      "\u001b[0;31mValueError\u001b[0m: Einstein sum subscript 'bs(hd)' does not contain the correct number of indices for operand 0.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 92\u001b[0m\n\u001b[1;32m     87\u001b[0m         params \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_put(params)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, params\n\u001b[0;32m---> 92\u001b[0m model, params \u001b[38;5;241m=\u001b[39m \u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSHARD_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSharded model initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m, in \u001b[0;36minit_model\u001b[0;34m(SHARD_MODEL)\u001b[0m\n\u001b[1;32m     58\u001b[0m         params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit(rng, jnp\u001b[38;5;241m.\u001b[39mones(input_shape, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint4))\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 60\u001b[0m         params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Load the parameters from the file\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:562\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__call__\u001b[0;34m(self, input_ids, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    564\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:426\u001b[0m, in \u001b[0;36mQwen2Model.__call__\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, deterministic, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    425\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 426\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:332\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.__call__\u001b[0;34m(self, hidden_states, attention_mask, position_ids, deterministic, output_attentions, position_embeddings)\u001b[0m\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Self-attention computation with GQA, RoPE, and QKV bias.\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Residual connection after attention.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/penzai/Torch2Jax-DeepSeek-R1-Distill-Qwen-1.5B/model_flax.py:200\u001b[0m, in \u001b[0;36mQwen2Attention.__call__\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, deterministic, output_attentions)\u001b[0m\n\u001b[1;32m    194\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(\n\u001b[1;32m    195\u001b[0m     hidden_states\n\u001b[1;32m    196\u001b[0m )  \u001b[38;5;66;03m# [batch, seq, num_key_value_heads * head_dim]\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Use einsum to reshape and transpose.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# For query_states: from [b, s, (nh * hd)] to [b, nh, s, hd]\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb s (h d) -> b h s d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# For key_states and value_states: from [b, s, (nk * cd)] to [b, nk, s, cd]\u001b[39;00m\n\u001b[1;32m    202\u001b[0m key_states \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s (k d) -> b k s d\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/mechinterp/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:9763\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(subscripts, out, optimize, precision, preferred_element_type, _dot_general, out_sharding, *operands)\u001b[0m\n\u001b[1;32m   9761\u001b[0m   contract_path \u001b[38;5;241m=\u001b[39m _poly_einsum_handlers\u001b[38;5;241m.\u001b[39mget(ty, _default_poly_einsum_handler)\n\u001b[1;32m   9762\u001b[0m \u001b[38;5;66;03m# using einsum_call=True here is an internal api for opt_einsum... sorry\u001b[39;00m\n\u001b[0;32m-> 9763\u001b[0m operands, contractions \u001b[38;5;241m=\u001b[39m \u001b[43mcontract_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9764\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_blas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9766\u001b[0m contractions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((a, \u001b[38;5;28mfrozenset\u001b[39m(b), c) \u001b[38;5;28;01mfor\u001b[39;00m a, b, c, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;129;01min\u001b[39;00m contractions)\n\u001b[1;32m   9768\u001b[0m einsum \u001b[38;5;241m=\u001b[39m jit(_einsum, static_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m), inline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mechinterp/lib/python3.11/site-packages/opt_einsum/contract.py:312\u001b[0m, in \u001b[0;36mcontract_path\u001b[0;34m(subscripts, use_blas, optimize, memory_limit, shapes, *operands, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m sh \u001b[38;5;241m=\u001b[39m input_shapes[tnum]\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sh) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(term):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEinstein sum subscript \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_list[tnum]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m does not contain the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect number of indices for operand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m     )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cnum, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(term):\n\u001b[1;32m    317\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sh[cnum])\n",
      "\u001b[0;31mValueError\u001b[0m: Einstein sum subscript 'bs(hd)' does not contain the correct number of indices for operand 0."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh\n",
    "from jax.sharding import NamedSharding\n",
    "\n",
    "import flax\n",
    "\n",
    "from rich import print\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch_to_flax import torch_to_flax\n",
    "from model_flax import get_partition_rules, Qwen2Config, Qwen2ForCausalLM\n",
    "\n",
    "# Set this environment variable before importing JAX.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9375\"\n",
    "\n",
    "\n",
    "SHARD_MODEL = True\n",
    "\n",
    "\n",
    "def create_sharding_specs(params):\n",
    "\n",
    "    # Get available JAX devices and create mesh\n",
    "    devices = jax.devices()\n",
    "    device_mesh = np.array(devices).reshape(-1)  # 1D mesh\n",
    "    mesh = Mesh(device_mesh, (\"mp\",))\n",
    "    print(mesh)\n",
    "\n",
    "    partition_rules = get_partition_rules()\n",
    "\n",
    "    def assign_spec(path, _):\n",
    "        # Create a slash-separated string from the tuple path\n",
    "        path_str = \"/\".join(map(str, path))\n",
    "        # Look for a matching partition rule\n",
    "        for rule_path, spec in partition_rules:\n",
    "            if rule_path in path_str:\n",
    "                return NamedSharding(mesh, spec)\n",
    "        # If no rule matches, return a default sharding spec\n",
    "        return NamedSharding(mesh, None)\n",
    "\n",
    "    \n",
    "    return jax.tree_util.tree_map_with_path(assign_spec, params)\n",
    "\n",
    "\n",
    "def init_model(SHARD_MODEL:bool):\n",
    "    \"\"\"Initialize a model with parameters on CPU then shard them to GPU after checkpoint loading.\"\"\"\n",
    "    # Create config and model\n",
    "    config = Qwen2Config()\n",
    "    model = Qwen2ForCausalLM(config=config)\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    input_shape = (1, 32)\n",
    "\n",
    "    # Force initialization on CPU to avoid duplicate GPU allocations\n",
    "    with jax.default_device(jax.devices(\"cpu\")[0]):\n",
    "        try:\n",
    "            params = model.init(rng, jnp.ones(input_shape, dtype=jnp.int4))\n",
    "        except Exception as e:\n",
    "            params = model.init(rng, jnp.ones(input_shape, dtype=jnp.int32))\n",
    "\n",
    "\n",
    "    # Load the parameters from the file\n",
    "    try:\n",
    "        with open(\"flax_params.msgpack\", \"rb\") as f:\n",
    "            params = {\n",
    "                \"params\": flax.serialization.from_bytes(params[\"params\"], f.read())\n",
    "            }\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Running conversion...\")\n",
    "        torch_to_flax()\n",
    "        with open(\"flax_params.msgpack\", \"rb\") as f:\n",
    "            params = {\n",
    "                \"params\": flax.serialization.from_bytes(params[\"params\"], f.read())\n",
    "            }\n",
    "\n",
    "    # Shard the parameters\n",
    "    if SHARD_MODEL:\n",
    "        sharding_specs = create_sharding_specs(params)\n",
    "\n",
    "        print(sharding_specs)\n",
    "\n",
    "        params = jax.tree_util.tree_map(\n",
    "            lambda x, spec: jax.device_put(x, spec), params, sharding_specs\n",
    "        )\n",
    "    else:\n",
    "        params = jax.device_put(params)\n",
    "\n",
    "\n",
    "    return model, params\n",
    "\n",
    "model, params = init_model(SHARD_MODEL)\n",
    "print(\"Sharded model initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generating tokens<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generating tokens\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Decoded text: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #000000; text-decoration-color: #000000\">｜begin▁of▁sentence｜&gt;What is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> + </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">? &lt;think&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">To solve </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> plus </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">, I start by identifying the numbers involved, which are </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Next, I add these two numbers together by combining them: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\"> and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000\"> make </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Therefore, the final answer is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #000000; text-decoration-color: #000000\">.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">think</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "\n",
       "To solve the addition \\<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\\<span style=\"font-weight: bold\">)</span>, follow these easy steps:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Identify the numbers to add:**\n",
       "   <span style=\"font-weight: bold\">[</span>\n",
       "   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \\quad \\text<span style=\"font-weight: bold\">{</span>and<span style=\"font-weight: bold\">}</span> \\quad <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Decoded text: \u001b[1m<\u001b[0m\u001b[39m｜begin▁of▁sentence｜>What is \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m + \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m? <think>\u001b[0m\n",
       "\u001b[39mTo solve \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m plus \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m, I start by identifying the numbers involved, which are \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m and \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39mNext, I add these two numbers together by combining them: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m and \u001b[0m\u001b[1;36m4\u001b[0m\u001b[39m make \u001b[0m\u001b[1;36m7\u001b[0m\u001b[39m.\u001b[0m\n",
       "\n",
       "\u001b[39mTherefore, the final answer is \u001b[0m\u001b[1;36m7\u001b[0m\u001b[39m.\u001b[0m\n",
       "\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mthink\u001b[0m\u001b[1m>\u001b[0m\n",
       "\n",
       "To solve the addition \\\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m + \u001b[1;36m4\u001b[0m\\\u001b[1m)\u001b[0m, follow these easy steps:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Identify the numbers to add:**\n",
       "   \u001b[1m[\u001b[0m\n",
       "   \u001b[1;36m3\u001b[0m \\quad \\text\u001b[1m{\u001b[0mand\u001b[1m}\u001b[0m \\quad \u001b[1;36m4\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "prompt = \"What is 3 + 4? <think>\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = jnp.array(inputs[\"input_ids\"].numpy())\n",
    "# Generate\n",
    "print(\"Generating tokens...\")\n",
    "output = model.generate(\n",
    "    params,\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    prng_key=jax.random.PRNGKey(0),\n",
    ")\n",
    "\n",
    "# Decode using your tokenizer\n",
    "decoded = tokenizer.decode(np.array(output[0]))\n",
    "print(\"Decoded text:\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}